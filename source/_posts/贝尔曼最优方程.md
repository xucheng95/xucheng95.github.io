---
title: 贝尔曼最优方程
date: 2023-06-06T10:57:08+08:00
tags: BOE
categories: 强化学习
mathjax: True
---

&emsp;&emsp;强化学习的最终目标是要找到最优策略(optimal policy)。本文将重点介绍贝尔曼最优方程(Bellman Optimality Equation, BOE)，能够帮助我们求解最优状态价值函数，从而可以得到一个最优策略，涉及到的知识点包括压缩映射定理(Contraction Mapping Theorem)。

## 压缩映射定理
### 基本定义
&emsp;&emsp;假设函数$f(x)$，其中$x \in \mathbb{R}^d$，$f:\mathbb{R}^d\rightarrow\mathbb{R}^d$。如果
$$
f(x^*)=x^*
$$
那么就称$x^*$是一个fixed point。对于函数$f$，如果$\forall x_1\in\mathbb{R}$，$\forall x_2\in\mathbb{R}$，存在$\gamma\in(0,1)$，使得
$$
\parallel f(x_1)-f(x_2)\parallel \le \gamma \parallel x_1-x_2\parallel 
$$
那么函数$f$可称为压缩映射函数，其中$\parallel \cdot\parallel$表示矩阵或者向量的范数。

&emsp;&emsp;**定义1：压缩映射定理** 对于具有$x=f(x)$形式的等式，其中$x$和$f(x)$均为实向量，如果$f$是一个压缩映射函数，那么就一定存在唯一的fixed point $x^*$。可以通过迭代的方法，求解出fiexed point.
  
### 定理证明

&emsp;&emsp;**证明1：序列$\{x_k=f(x_{k-1})\}_{k=1}^\infty$的收敛性。** 

&emsp;&emsp;由于函数$f$是一个压缩映射函数，因此有
$$
\parallel x_{k+1}-x_k\parallel =\parallel f(x_k)-f(x_{k-1}) \parallel \le\gamma \parallel x_k-x_{k-1} \parallel 
$$

同样的，可以得到 $\parallel x_k-x_{k-1} \parallel \le \gamma \parallel x_{k-1}-x_{k-2} \parallel \le...\le\gamma\parallel x_1-x_0 \parallel$。因此，

$$
\begin{aligned}
\parallel x_{k+1}-x_k\parallel  &\le \gamma \parallel x_k-x_{k-1}\parallel  \\
                &\le \gamma^2\parallel x_{k-1}-x_{k-2}\parallel  \\
                &\vdots \\
                &\le \gamma^k\parallel x_1-x_0\parallel  \\
\end{aligned}
$$
由于$\gamma<1$，所以$\parallel x_{k+1}-x_k\parallel$会随着$k\rightarrow\infty$以指数收敛到0。进一步考虑$m>n$的情况，有
$$
\begin{aligned}
\parallel x_m-x_n\parallel  &=\parallel x_m-x_{m-1}+x_{m-1}-\dots-x_{n+1}+x_{n+1}-x_n\parallel \\
            &\le\parallel x_m-x_{m-1}\parallel +\dots+\parallel x_{n+1}-x_{n}\parallel \\
            &\le\gamma^{m-1}\parallel x_1-x_0\parallel +\dots+\gamma^n\parallel x_1-x_0\parallel \\
            &=\gamma^n(\gamma^{m-1-n}+\dots+1)\parallel x_1-x_0\parallel \\
            &\le\frac{\gamma^n}{1-\gamma}\parallel x_1-x_0\parallel 
\end{aligned}
$$
因此，对于任意的$\epsilon$，都可以找到$N$使得$\parallel x_m-x_n\parallel <\epsilon$。所以$\{x_k\}$是一个柯西序列，最终会收敛到一个limit point，$x^*=\lim_{k\rightarrow\infty}x_k$。

&emsp;&emsp;**证明2：limit point $x^*=\lim_{k\rightarrow\infty}x_k$ 就是fixed point。**

&emsp;&emsp;由于
$$
\parallel f(x_k)-x_k\parallel =\parallel x_{k+1}-x_k\parallel \le\gamma^k\parallel x_1-x_0\parallel 
$$
$\parallel f(x_k)-x_k\parallel$会以指数快速收敛到0，所以可以得到$f(x^*)=x^*$

&emsp;&emsp;**证明3：fixed point 是唯一的。**

&emsp;&emsp;假设存在另一个fixed point $x'$满足$f(x')=x'$，那么
$$
\parallel x'-x^*\parallel =\parallel f(x')-f(x^*)\parallel \le\gamma\parallel x'-x^*\parallel 
$$
由于$\gamma<1$，不等式只能在$\parallel x'-x^*\parallel =0$时成立，所以fixed point唯一。

&emsp;&emsp;**证明4：$x_k$以指数速度收敛**

&emsp;&emsp;根据前面的证明$\parallel x_m-x_n \parallel\le\frac{\gamma^n}{1-\gamma}\parallel x_1-x_0\parallel$，$m$可以取一个任意大的值，于是可以得到
$$
\parallel x^*-x_n\parallel=\lim_{m\rightarrow\infty}\parallel x_m-x_n\parallel\le\frac{\gamma^n}{1-\gamma}\parallel x_1-x_0\parallel
$$
$\gamma<1$，所以误差会以指数收敛到0，并且$\gamma$越小，收敛得也就越快。

## 最优状态策略与最优状态价值
&emsp;&emsp;状态价值函数是对策略优劣的一种评估，假如$\forall s \in \mathcal{S}$， $v_{\pi_{1}}(s)>v_{\pi_{2}}(s)$，则可以认为策略$\pi_1$要优于$\pi_2$。同样的，假如一个策略要是优于其他所有策略，那么该策略就是最优策略。

&emsp;&emsp;**定义2：最优策略与最优状态价值** 对于所有的状态$s\in\mathcal{S}$和任意的策略$\pi$，都有$v_{\pi^*}\ge v_\pi$，那么$\pi^*$就是最优策略，它的状态价值就是最优状态价值。

&emsp;&emsp;为了求解最优状态价值，得到最优策略，我们需要一个求解工具——贝尔曼最优方程(Bellman Optimality Equation)

## 贝尔曼最优方程
### 方程的形式
&emsp;&emsp;贝尔曼最优方程：
$$
\begin{aligned}
v(s)&=\max_\pi\sum_a\pi(a|s)\left( \sum_rp(r|s,a)r+\gamma\sum_{s'}p(s'|s,a)v(s') \right) \\
    &=\max_\pi\sum_a\pi(a|s)q(s,a)
\end{aligned}
$$
其中，$v(s)$与$v(s')$都是未知的，
$$
q(s,a)\doteq\sum_r p(r|s,a)r+\gamma\sum_{s'}p(s'|s,a)v(s')
$$

&emsp;&emsp;BOE的矩阵形式：
$$
v = \max_\pi(r_\pi+\gamma P_\pi v)
$$
其中$v\in \mathbb{R}^{|\mathcal{S}|}$。将右边用$f(v)$定义，那么BOE就变为
$$
v=f(v)
$$

### 方程的求解
&emsp;&emsp;根据压缩映射定理，假设$v^*$是$v=f(v)=\max_\pi(r_\pi+\gamma P_\pi v)$的解，那么显然$v^*$是一个fixed point。当使用迭代法
$$
v_{k+1}=f(v_k)=\max_{\pi}(r_{\pi}+\gamma P_{\pi}v_k)
$$
求解时，$\{v_k\}$以指数收敛到$v^*$。

&emsp;&emsp;有了$v^*$后，可以通过
$$
\pi^* = \arg\max_\pi(r_\pi+\gamma P_{\pi^*}v^*)
$$
得到对应的策略$\pi^*$。此时$v^*$和$\pi^*$满足
$$
v^*=r_{\pi^*}+\gamma P_{\pi^*}v^*
$$
因此，$v^*=v_{\pi^*}$是策略$\pi^*$的状态价值，BOE就是基于策略$\pi^*$的贝尔曼方程。

