---
title: 强化学习中的价值估计与优势函数计算方法
date: 2024-06-25 11:23:58
tags: 强化学习基础
categories: 强化学习
mathjax: True
---


## 1. 价值估计方法

### 1.1 折扣回报 (Discounted Return)

折扣回报方法通过计算从某个状态开始直到回合结束的累计折扣奖励来估计价值函数。其定义为：

$$ G_t = \sum_{k=0}^{\infty} \gamma^k r_{t+k} $$

其中：
- $\gamma$ 是折扣因子，通常 $0 \leq \gamma \leq 1 $。
- $ r_t$ 是时间步 $t$ 的即时奖励。

#### 优点

1. **无偏估计**：折扣回报利用了整个轨迹的信息，提供了回报的无偏估计。
2. **全局信息**：考虑了从当前状态到回合结束的所有奖励，能够反映长期的回报情况。

#### 缺点

1. **高方差**：受个别奖励波动的影响较大，容易产生高方差的估计。
2. **延迟反馈**：必须等待整个回合结束后才能计算完整的折扣回报，导致反馈延迟，不适用于持续任务或长回合任务。
3. **计算复杂**：对于较长的回合，需要储存和处理大量的状态和奖励信息，计算复杂度较高。

### 1.2 时间差分 (Temporal Difference, TD)

时间差分方法结合了动态规划的思想，通过利用当前奖励和下一状态的估计值来逐步更新价值函数。TD(0) 的更新公式为：

$$ V(s_t) \leftarrow V(s_t) + \alpha \left( r_{t+1} + \gamma V(s_{t+1}) - V(s_t) \right) $$

其中：
- $\alpha$ 是学习率。
- $r_{t+1}$ 是当前奖励。
- $V(s_{t+1})$ 是下一状态的估计值。

#### 优点

1. **低方差**：每一步都更新，TD 方法的更新方差较低。
2. **即时更新**：不需要等待整个回合结束，可以在线更新，适用于持续任务。
3. **计算高效**：更新只需要当前状态、即时奖励和下一状态的估计值，计算复杂度较低。

#### 缺点

1. **偏差问题**：由于使用了估计值进行更新，TD 方法可能产生偏差，依赖于价值函数的初始估计。
2. **局部信息**：仅依赖于当前和下一步的信息，可能无法反映长期回报的全貌。

### 1.3 TD(λ) 方法

为了结合折扣回报和时间差分方法的优点，引入了 TD(λ) 方法，通过参数 $\lambda$ 在单步和多步时间差分之间进行权衡：

$$ V(s_t) \leftarrow V(s_t) + \alpha \sum_{n=0}^{\infty} (\lambda \gamma)^n \delta_{t+n} $$

其中时间差分误差 $\delta_t$ 定义为：

$$ \delta_t = r_{t+1} + \gamma V(s_{t+1}) - V(s_t) $$

#### 优点

1. **灵活性**：通过调整 $\lambda$，可以在低方差和无偏估计之间进行权衡。
2. **平衡偏差与方差**：结合了折扣回报和时间差分的优点，在实践中表现出色。

### 1.4 综合比较

| 方法           | 估计方式                  | 更新时机           | 优点                         | 缺点                          |
| -------------- | ------------------------- | ------------------ | ---------------------------- | ----------------------------- |
| 折扣回报       | 完整轨迹的折扣回报       | 回合结束后         | 无偏估计，全局信息           | 高方差，延迟反馈，计算复杂    |
| 时间差分       | 当前奖励和下一状态估计   | 每一步             | 低方差，即时更新，计算高效   | 偏差问题，局部信息            |
| TD(λ)         | 结合折扣回报和时间差分    | 每一步             | 灵活性，平衡偏差与方差       | 需要选择合适的 $\lambda$，计算复杂度增加 |

---

## 2. 优势函数的计算

优势函数 $A(s, a)$ 衡量在状态 $s$ 下采取动作 $a$ 的优劣程度，其定义为：

$$ A(s, a) = Q(s, a) - V(s) $$

其中：
- $Q(s, a)$ 是状态-动作值函数，表示在状态 $s$ 采取动作 $a$ 后的期望回报。
- $V(s)$ 是状态值函数，表示在状态 $s$ 的期望回报。

### 2.1 单步时间差分 (One-Step TD Advantage)

直接利用当前的即时奖励和下一状态的价值来计算优势函数：

$$ \hat{A}_t = r_t + \gamma V(s_{t+1}) - V(s_t) $$

### 2.2 多步时间差分 (n-Step TD Advantage)

通过考虑多步回报来计算优势函数，可以减少方差：

$$ \hat{A}_t = \sum_{i=0}^{n-1} \gamma^i r_{t+i} + \gamma^n V(s_{t+n}) - V(s_t) $$

### 2.3 广义优势估计 (Generalized Advantage Estimation, GAE)

GAE 是一种更为通用和高效的方法，通过引入衰减参数 $\lambda$，在单步和多步时间差分之间进行权衡：

$$ \hat{A}_t^{GAE(\gamma, \lambda)} = \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l} $$

其中时间差分误差 $\delta_t$ 定义为：

$$ \delta_t = r_t + \gamma V(s_{t+1}) - V(s_t) $$

#### 优点

1. **灵活性**：通过调整 $\lambda$，GAE 可以在偏差和方差之间取得平衡。
2. **稳定性和准确性**：GAE 对未来的优势进行加权平均，提供更稳定和准确的优势估计。

### 2.4 伪代码示例

以下是计算优势函数的伪代码示例，包括单步 TD、n 步 TD 和 GAE 方法。

```python
import numpy as np

def compute_advantages(rewards, values, gamma, lambda_=None):
    T = len(rewards)
    advantages = np.zeros(T)
    if lambda_ is None:
        # One-step TD advantage
        for t in range(T):
            if t == T - 1:
                next_value = 0  # Episode end
            else:
                next_value = values[t + 1]
            advantages[t] = rewards[t] + gamma * next_value - values[t]
    else:
        # Generalized Advantage Estimation (GAE)
        deltas = np.zeros(T)
        for t in range(T):
            if t == T - 1:
                next_value = 0  # Episode end
            else:
                next_value = values[t + 1]
            deltas[t] = rewards[t] + gamma * next_value - values[t]
        
        # GAE computation
        for t in reversed(range(T)):
            if t == T - 1:
                advantages[t] = deltas[t]
            else:
                advantages[t] = deltas[t] + gamma * lambda_ * advantages[t + 1]
    return advantages

# Example usage
rewards = [1, 1, 1, 1, 1]
values = [0.5, 0.6, 0.7, 0.8, 0.9]
gamma = 0.99
lambda_ = 0.95  # For GAE

# Compute GAE advantages
advantages = compute_advantages(rewards, values, gamma, lambda_)
print("GAE Advantages:", advantages)

# Compute one-step TD advantages
advantages_one_step = compute_advantages(rewards, values, gamma)
print("One-step TD Advantages:", advantages_one_step)
```

---

## 3. 总结

在强化学习中，价值估计和优势函数的计算方法对策略优化的效果有重要影响。折扣回报和时间差分方法是估计价值函数的两种主要技术，各有优缺点。折扣回报方法提供无偏的长远回报估计，但计算复杂且方差较高；时间差分方法通过局部更新，降低了计算复杂度和方差，但可能引入偏差。结合两者优点的 TD($\lambda$) 方法提供了更灵活的估计方式。

在计算优势函数时，单步 TD、多步 TD 和广义优势估计（GAE）是常用的方法。GAE 方法通过引入 $\lambda$ 参数，在偏差和方差之间取得平衡，是现代强化学习算法如 PPO 的推荐选择。

理解这些方法的优缺点，并根据具体任务和环境选择合适的方法，能够显著提升强化学习算法的性能和稳定性。