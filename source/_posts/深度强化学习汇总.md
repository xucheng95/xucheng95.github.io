---
title: 深度强化学习汇总
date: 2024-04-14T22:37:10+08:00
tags: 深度强化学习
categories: 强化学习
mathjax: True
---

# [DQN](https://arxiv.org/abs/1312.5602)
&emsp;&emsp;深度Q网络（Deep Q-Network，DQN）是一种结合了深度学习与强化学习的算法，用于解决复杂的决策和控制问题。DQN算法由Google DeepMind团队提出，旨在通过深度神经网络（DNN）来逼近Q值函数，从而实现对高维状态空间的有效处理。

## DQN的核心概念
1. Q学习（Q-Learning）\
    &emsp;&emsp; Q学习是一种无模型的强化学习算法，通过学习一个动作价值函数（Q函数）来估计在给定状态下执行某个动作的期望回报。Q学习的更新公式为：
    $$Q(s,a) \leftarrow Q(s,a) + \alpha \left[ r + \gamma \max_{a'} Q(s',a') - Q(s,a) \right]$$
    其中，$s$和$a$分别为当前状态和动作，$r$为即时奖励，$s'$为下一状态，$\gamma$为折扣因子，$\alpha$为学习率。
2. 深度神经网络（Deep Neural Network，DNN）\
    &emsp;&emsp;深度神经网络被用于逼近$Q$值函数，以处理高维的状态空间。DNN通过输入状态输出各个动作的$Q$值，从而选择最优动作
3. 经验回放（Experience Replay）\
    &emsp;&emsp;为了打破样本间的相关性，DQN使用经验回放机制。它将代理的经验（状态、动作、奖励、下一状态）存储在回放缓冲区中，并从中随机抽取小批量样本进行训练，从而提高样本的利用率和训练的稳定性。
4. 目标网络（Target Network）\
    &emsp;&emsp;DQN引入了目标网络来稳定训练过程。目标网络的参数定期更新为当前Q网络的参数，用于计算目标$Q$值，避免了直接使用当前网络参数导致的不稳定性。

## DQN算法流程
1. 初始化：
   * 初始化经验回放缓冲区。
   * 初始化Q网络和目标网络的参数。
2. 循环训练：
   * 在当前状态下，根据epsilon-贪婪策略选择动作（即以概率epsilon随机选择动作，以1-epsilon的概率选择当前Q网络认为最优的动作）。
   * 执行动作，获得奖励并转移到下一状态。
   * 将经验（状态、动作、奖励、下一状态）存储到经验回放缓冲区。
   * 从经验回放缓冲区随机抽取小批量样本，计算目标$Q$值：
    $$y=r+\gamma\max_{a'}Q_{target}(s',a')$$
   * 使用梯度下降法最小化Q网络的损失函数：
    $$L(\theta)=\mathbb{E}[(y-Q(s, a;\theta))^2]$$
   * 定期将Q网络的参数复制到目标网络。

## DQN的优点
1. 能够处理高维的状态空间，适用于复杂的环境。
2. 通过经验回放和目标网络提高了训练的稳定性和效率。

# [Double DQN](https://arxiv.org/abs/1509.06461)
&emsp;&emsp;Double DQN（Double Deep Q-Network）是一种改进的深度强化学习算法，旨在解决DQN（Deep Q-Network）中的$Q$值过度估计问题。它的核心思想是通过分离动作选择和动作评估的步骤来减轻这一问题。

## Double DQN的核心概念
1. 分离动作选择与动作评估\
   &emsp;&emsp;在计算目标$Q$值时，Double DQN使用当前Q网络选择下一个动作，但使用目标Q网络评估该动作的$Q$值。这种分离减少了最大化操作导致的过度估计问题。
2. 目标值计算\
   &emsp;&emsp;Double DQN的目标值计算为：
   $$y=r+\gamma Q(s', \argmax _{a'} Q(s',a';\theta);\theta^-)$$
   其中，$\theta$是当前Q网络的参数，$\theta^-$是目标Q网络的参数。

## Double DQN的算法流程
1. 初始化：
   * 初始化经验回放缓冲区。
   * 初始化Q网络和目标网络的参数。
2. 循环训练： 
    * 在当前状态下，根据epsilon-贪婪策略选择动作（即以概率epsilon随机选择动作，以1-epsilon的概率选择当前Q网络认为最优的动作）。
   * 执行动作，获得奖励并转移到下一状态。
   * 将经验（状态、动作、奖励、下一状态）存储到经验回放缓冲区。
   * 从经验回放缓冲区随机抽取小批量样本，计算目标$Q$值：
    $$y=r+\gamma Q(s', \argmax _{a'} Q(s',a';\theta);\theta^-)$$
   * 使用梯度下降法最小化Q网络的损失函数：
    $$L(\theta)=\mathbb{E}[(y-Q(s, a;\theta))^2]$$
   * 定期将Q网络的参数复制到目标网络。

## Double DQN的优点
1. 减少过度估计：通过分离动作选择和动作评估，Double DQN有效减少了$Q$值的过度估计问题，从而提高了训练的稳定性和策略的性能。
2. 稳定性更强：相比于DQN，Double DQN在复杂环境中的表现更加稳定，训练过程中的波动较少。
3. 改进的策略质量：减少过度估计问题使得策略的选择更加准确，导致更高质量的策略。

# [Prioritized Replay DQN](https://arxiv.org/pdf/1511.05952)
&emsp;&emsp;Prioritized Replay DQN（优先经验回放的深度Q网络）是对DQN算法的改进版本，它通过引入优先经验回放机制来提升训练效率和性能。优先经验回放的核心思想是优先采样那些误差较大的经验进行训练，因为这些经验包含了更多的信息，能够更有效地更新$Q$网络。

## Prioritized Replay DQN的核心概念
1. TD误差\
   &emsp;&emsp;时间差分（TD）误差是当前$Q$值与目标$Q$值之间的差异。优先经验回放通过TD误差来确定每个经验的优先级。
2. 优先级采样\
   &emsp;&emsp;经验的采样概率与其TD误差相关，TD误差越大，经验被采样的概率越高。
3. 重要性采样权重\
   &emsp;&emsp;为了修正优先采样引入的偏差，引入了重要性采样权重，在更新时对不同经验赋予不同的权重。

## Prioritized Replay DQN的算法流程
1. 初始化：
   * 初始化经验回放缓冲区。
   * 初始化Q网络和目标网络的参数。
   * 初始化优先级树（SumTree）或其他数据结构来存储经验优先级。
2. 循环训练： 
   * 从当前状态$s$选择动作$a$（使用$\epsilon$-贪婪策略）。
   * 执行动作$a$，观察奖励$r$和下一状态$s'$。
   * 将经验($s$, $a$, $r$, $s'$)存储到经验回放缓冲区，并计算其优先级（通常使用初始优先级）。
   * 从经验回放缓冲区中按优先级采样小批量经验，根据TD误差计算每个经验的采样权重。
   * 计算目标$Q$值并更新Q网络的参数。
   * 使用采样权重修正更新中的偏差。
   * 更新经验的优先级。

## Prioritized Replay DQN的优点
1. 提升训练效率：通过优先采样那些TD误差较大的经验，Prioritized Replay DQN能够更有效地利用经验数据，快速纠正高误差的状态-动作对，从而加速训练过程。
2. 更快收敛：因为TD误差大的经验通常包含更多的信息，优先采样这些经验能够更快地修正Q网络的参数，使得训练过程中的Q函数逼近更为准确，收敛速度更快。
3. 改进的策略性能：通过优先更新那些对策略影响较大的经验，Prioritized Replay DQN能够更迅速地提升策略的性能，尤其是在复杂和动态变化的环境中。
4. 减少资源浪费：普通经验回放对所有经验一视同仁，可能会浪费大量资源在无关紧要或重复的经验上。Prioritized Replay DQN通过选择重要的经验进行训练，减少了资源浪费。


